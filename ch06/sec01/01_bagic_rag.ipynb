{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec930df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS 인덱스 faiss_index를 로드합니다.\n",
      "[결과 1]\n",
      "모 데이터를 효과적으로 처리하는 데 한계가 있었고, 이는 \n",
      "임상시험 과정에서 오류와 지연을 초래할 수 있었다. LLM \n",
      "기반 생성형 AI는 이러한 문제를 해결할 수 있는 가능성을 \n",
      "보여주고 있다. 특히 LLM은 기존 통계적 방법론과 달리, \n",
      "데이터의 양과 복잡성에 구애받지 않고 패턴을 분석하고 \n",
      "중요한 정보를 추출할 수 있다. \n",
      "본 연구는 LLM 기반 생성형 AI가 임상시험에 어떻게 \n",
      "적용될 수 있는지에 대해 구체적인 방법을 제시하고, 이를 \n",
      "통해 임상시험의 신뢰성과 효율성을 어떻게 향상시킬 수 \n",
      "있는지를 분석한다 [10]. 구체적\n",
      "---\n",
      "[결과 2]\n",
      "정밀 의료, 임상시험 관리 자동화, 그리고 도메인 지식 기반의 질의응답 시스템에서 Private LLM\n",
      "의 실질적 활용 가능성을 확인하였다.\n",
      "▸주제어: 대규모 언어 모델, 생성형 인공지능, 임상시험, 의료기기, 데이터 분석\n",
      "I. Introduction\n",
      "대규모 언어 모델(Large Language Models, LLM)의 \n",
      "발전은 최근 몇 년간 인공지능(AI) 기술의 중요한 진전으\n",
      "로 자리 잡고 있다 [1]. LLM 기반 생성형 AI는 자연어 처\n",
      "리와 텍스트 생성, 데이터 분석 등 다양한 분야에서 뛰어\n",
      "난 성능을 보이고 있으며, 특히\n",
      "---\n",
      "[결과 3]\n",
      "터를 효과적으로 처리할 수 있도록 조정한다 [8]. \n",
      "LLM이 학습된 후, 이를 임상시험 과정에 통합하여 실\n",
      "시간으로 데이터를 분석하고 그 결과를 도출한다. 학습된 \n",
      "LLM은 임상 데이터를 처리하고, 패턴을 찾아내며, 결과를 \n",
      "예측하는 데 중요한 역할을 한다. 특히, 자동화된 분석 과\n",
      "정을 통해 임상시험의 속도를 높이고, 신뢰할 수 있는 결\n",
      "과를 빠르게 도출하는 것이 가능하다. 이러한 기술적 구현\n",
      "은 임상시험의 효율성을 극대화하는 데 기여할 수 있다.\n",
      "2.5 A method of research\n",
      "본 연구에서는 여러 사례 연구를 통해\n",
      "---\n",
      "[결과 4]\n",
      "였다.\n",
      "2. Related works\n",
      "2.1 Domestic trends\n",
      "한국에서는 대규모 언어 모델(LLM)을 임상시험에 적용\n",
      "하는 연구가 학계와 연구 기관을 중심으로 활발히 진행되\n",
      "고 있다. KAIST, 서울대학교, 연세대학교와 같은 주요 대\n",
      "학들이 의료 데이터 분석과 예측 모델링에 LLM을 통합하\n",
      "기 위한 노력을 주도하고 있다. 예를 들어, KAIST는 대규\n",
      "모 임상 데이터를 분석해 질병 결과를 예측하는 LLM 기반 \n",
      "모델을 개발했으며, 이 모델은 향후 임상시험에서 중요한 \n",
      "역할을 할 것으로 예상된다. \n",
      "한국보건산업진흥원(KH\n",
      "---\n",
      "[결과 5]\n",
      "의료기기 임상시험 분야의 도메인 특성에 맞게 튜닝하\n",
      "기 위해 의료기기 임상시험 전문가로부터 총 158개의 문\n",
      "서(총 11,954 페이지)를 수집하였다. 수집된 문서는 다음\n",
      "과 같이 분류된다:\n",
      " 규제 문서 (30%): FDA, EMA, PMDA 가이드라인, \n",
      "GCP 문서 등\n",
      " 교육 자료 (20%): 임상시험 수행자 교육 매뉴얼, 온라\n",
      "인 강의 자료 등\n",
      " 프로토콜 및 보고서 (25%): 임상시험 프로토콜, CSR \n",
      "(Clinical Study Report) 템플릿 등\n",
      " 의료기기 특화 문서 (15%): 의료기기 임상시험 계획\n",
      "서,\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# 문서 로드\n",
    "loader = PyPDFLoader('../data/KCI_FI003153549.pdf')\n",
    "documents = loader.load()\n",
    "\n",
    "# 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# 임베딩 모델 준비\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=gemini_api_key,\n",
    ")\n",
    "\n",
    "# # FAISS 벡터스토어 생성 및 저장\n",
    "# vectorstore = FAISS.from_documents(splitted_documents, embedding_model)\n",
    "# vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "# # 벡터스토어 재로딩\n",
    "# reloaded_store = FAISS.load_local(\n",
    "#     \"faiss_index\",\n",
    "#     embedding_model,\n",
    "#     allow_dangerous_deserialization=True,\n",
    "# )\n",
    "\n",
    "# FAISS 벡터스토어가 존재하는 경우에는 덮어쓰기 하지 않고 로드\n",
    "FAISS_INDEX_PATH = \"faiss_index\"\n",
    "\n",
    "if os.path.exists(FAISS_INDEX_PATH):\n",
    "    print(f\"FAISS 인덱스 {FAISS_INDEX_PATH}를 로드합니다.\")\n",
    "    reloaded_store = FAISS.load_local(\n",
    "        FAISS_INDEX_PATH,\n",
    "        embedding_model,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "else:\n",
    "    print(f\"FAISS 인덱스 {FAISS_INDEX_PATH}가 없으므로 생성합니다.\")\n",
    "    # 문서 로드\n",
    "    loader = PyPDFLoader('../data/KCI_FI003153549.pdf')\n",
    "    documents = loader.load()\n",
    "\n",
    "    # 문서 분할\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    splitted_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "    # FAISS 벡터스토어 생성 및 저장\n",
    "    vectorstore = FAISS.from_documents(splitted_documents, embedding_model)\n",
    "    vectorstore.save_local(FAISS_INDEX_PATH)\n",
    "    print(f\"FAISS 인덱스를 {FAISS_INDEX_PATH}에 저장했습니다.\")\n",
    "\n",
    "# 예시 질의\n",
    "query = \"본 연구에서 Private LLM 구축을 위해 수집한 문서의 총 페이지 수와 문서 유형별 비율은 어떻게 되나요?\"\n",
    "# query = \"Advance RAG 기법이 임상시험 데이터 분석에서 수행하는 주요 역할은 무엇인가요?\"\n",
    "# query = \"본 연구에서 Private LLM 성능을 평가하기 위해 사용한 지표 3가지는 무엇인가요?\"\n",
    "# query = \"국내에서 LLM을 임상시험에 적용한 대표적인 기관과 그 적용 사례를 2가지 이상 말해보세요.\"\n",
    "# query = \"ROUGE 평가에서 Private LLM과 ChatGPT의 Recall 값은 각각 얼마였나요?\"\n",
    "\n",
    "# results = reloaded_store.similarity_search(query, k=3) # k는 유사도 검색에서 반환할 상위 문서 개수(top‑k)\n",
    "results = reloaded_store.similarity_search(query, k=5)\n",
    "\n",
    "for idx, doc in enumerate(results, start=1):\n",
    "    print(f\"[결과 {idx}]\\n\" + doc.page_content[:300])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "815659bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='본 연구에서 Private LLM 구축을 위해 수집한 문서의 총 페이지 수는 **11,954 페이지**입니다.\\n\\n수집된 문서 유형별 비율은 다음과 같습니다:\\n*   **규제 문서**: 30%\\n*   **교육 자료**: 20%\\n*   **프로토콜 및 보고서**: 25%\\n*   **의료기기 특화 문서**: 15%\\n*   **기타**: 10%' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--4bf49da3-6559-4b24-9974-7c726f0955f2-0' usage_metadata={'input_tokens': 2684, 'output_tokens': 466, 'total_tokens': 3150, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 361}}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    '''다음 컨텍스트만 사용해 질문에 답하세요.\n",
    "컨텍스트:{context}\n",
    "\n",
    "질문: {question}\n",
    "'''\n",
    ")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=gemini_api_key)\n",
    "llm_chain = prompt | llm\n",
    "\n",
    "# 관련 문서를 사용한 답변\n",
    "result = llm_chain.invoke({'context': results, 'question': query})\n",
    "\n",
    "print(result)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e630fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'본 연구에서 Private LLM 구축을 위해 수집한 문서의 총 페이지 수와 문서 유형별 비율은 어떻게 되나요?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43658371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "본 연구에서 Private LLM 구축을 위해 수집한 문서의 총 페이지 수는 **11,954 페이지**입니다.\n",
      "\n",
      "문서 유형별 비율은 다음과 같습니다:\n",
      "*   **규제 문서**: 30%\n",
      "*   **교육 자료**: 20%\n",
      "*   **프로토콜 및 보고서**: 25%\n",
      "*   **의료기기 특화 문서**: 15%\n",
      "*   **기타**: 10%\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "# 데코레이터 @chain는 이 함수를 LangChain runnable로 변환하여\n",
    "# LangChain의 chain 작업과 파이프라인과 호환되게 함\n",
    "@chain\n",
    "def qa(input):\n",
    "    # 관련 문서 검색\n",
    "    docs = reloaded_store.similarity_search(input, k=5)\n",
    "    # 프롬프트 포매팅\n",
    "    formatted = prompt.invoke({'context': docs, 'question': input})\n",
    "    # 답변 생성\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "    # return {\"answer\": answer, \"docs\": docs}\n",
    "\n",
    "# 실행\n",
    "result = qa.invoke(query)\n",
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
