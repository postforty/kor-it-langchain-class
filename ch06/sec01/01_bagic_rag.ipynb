{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dec930df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS 인덱스 faiss_index를 로드합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# 문서 로드\n",
    "loader = PyPDFLoader('../data/KCI_FI003153549.pdf')\n",
    "documents = loader.load()\n",
    "\n",
    "# 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# 임베딩 모델 준비\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-001\",\n",
    "    google_api_key=gemini_api_key,\n",
    ")\n",
    "\n",
    "# # FAISS 벡터스토어 생성 및 저장\n",
    "# vectorstore = FAISS.from_documents(splitted_documents, embedding_model)\n",
    "# vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "# # 벡터스토어 재로딩\n",
    "# reloaded_store = FAISS.load_local(\n",
    "#     \"faiss_index\",\n",
    "#     embedding_model,\n",
    "#     allow_dangerous_deserialization=True,\n",
    "# )\n",
    "\n",
    "# FAISS 벡터스토어가 존재하는 경우에는 덮어쓰기 하지 않고 로드\n",
    "FAISS_INDEX_PATH = \"faiss_index\"\n",
    "\n",
    "vectorstore = None\n",
    "\n",
    "if os.path.exists(FAISS_INDEX_PATH):\n",
    "    print(f\"FAISS 인덱스 {FAISS_INDEX_PATH}를 로드합니다.\")\n",
    "    vectorstore = FAISS.load_local(\n",
    "        FAISS_INDEX_PATH,\n",
    "        embedding_model,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "else:\n",
    "    print(f\"FAISS 인덱스 {FAISS_INDEX_PATH}가 없으므로 생성합니다.\")\n",
    "    # 문서 로드\n",
    "    loader = PyPDFLoader('../data/KCI_FI003153549.pdf')\n",
    "    documents = loader.load()\n",
    "\n",
    "    # 문서 분할\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    splitted_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "    # FAISS 벡터스토어 생성 및 저장\n",
    "    vectorstore = FAISS.from_documents(splitted_documents, embedding_model)\n",
    "    vectorstore.save_local(FAISS_INDEX_PATH)\n",
    "    print(f\"FAISS 인덱스를 {FAISS_INDEX_PATH}에 저장했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37f1635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1d4704455b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb917fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[결과 1]\n",
      "의료기기 임상시험 분야의 도메인 특성에 맞게 튜닝하\n",
      "기 위해 의료기기 임상시험 전문가로부터 총 158개의 문\n",
      "서(총 11,954 페이지)를 수집하였다. 수집된 문서는 다음\n",
      "과 같이 분류된다:\n",
      " 규제 문서 (30%): FDA, EMA, PMDA 가이드라인, \n",
      "GCP 문서 등\n",
      " 교육 자료 (20%): 임상시험 수행자 교육 매뉴얼, 온라\n",
      "인 강의 자료 등\n",
      " 프로토콜 및 보고서 (25%): 임상시험 프로토콜, CSR \n",
      "(Clinical Study Report) 템플릿 등\n",
      " 의료기기 특화 문서 (15%): 의료기기 임상시험 계획\n",
      "서,\n",
      "---\n",
      "[결과 2]\n",
      "174   Journal of The Korea Society of Computer and Information \n",
      "문서(10%)로 구성되며, 각 분류는 도메인 전문가의 검토를 \n",
      "통해 정확성과 신뢰성을 확보하였다. 특히, 주요 규제 기\n",
      "관(FDA, EMA, PMDA) 가이드라인, GCP 문서, 환자 동의\n",
      "서 템플릿 등은 모델이 국제 표준에 기반하여 학습할 수 \n",
      "있도록 하였고, 교육 자료와 프로토콜은 실질적인 임상시\n",
      "험 수행과 데이터 관리 작업에서 발생할 수 있는 질문들에 \n",
      "대응할 수 있는 기초를 제공한다.\n",
      "이 데이터셋은 다양한 문서 \n",
      "---\n",
      "[결과 3]\n",
      "This study explores the improvement of work efficiency and expertise by applying Private LLM \n",
      "based on Large Language Model (LLM) to the field of clinical trials in medical devices. The Private \n",
      "LLM system provides sophisticated and accurate answers based on clinical data and shows its potential \n",
      "fo\n",
      "---\n",
      "[결과 4]\n",
      "170   Journal of The Korea Society of Computer and Information \n",
      "[요   약]\n",
      "본 연구는 대규모 언어 모델(LLM) 기반의 Private LLM을 활용하여 의료기기 임상시험 분야에 \n",
      "적용하여 업무 효율성과 전문성 향상을 탐구한다. Private LLM 시스템은 임상 데이터를 기반으로 \n",
      "정교하고 정확한 답변을 제공하며, 의사결정 지원, 임상 전문가 활동 보조, 새로운 콘텐츠 생성, \n",
      "문제 해결 등 다양한 응용 분야에서 활용 가능성을 보여준다. 연구는 다음 네 가지 주요 단계로 \n",
      "구성된\n",
      "---\n",
      "[결과 5]\n",
      "Automated processes reduce the time \n",
      "required for analysis.\n",
      "Detailed \n",
      "Insights\n",
      "LLM provides detailed insights to \n",
      "support medical decision-making.\n",
      "Scalability LLM can handle large datasets smoothly.\n",
      "Table 5. Key Benefits of LLM-Based Analysis in   \n",
      "Medical Device Clinical Trials\n",
      "III. The Proposed Sc\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# 예시 질의\n",
    "query = \"본 연구에서 Private LLM 구축을 위해 수집한 문서의 총 페이지 수와 문서 유형별 비율은 어떻게 되나요?\"\n",
    "# query = \"Advance RAG 기법이 임상시험 데이터 분석에서 수행하는 주요 역할은 무엇인가요?\"\n",
    "# query = \"본 연구에서 Private LLM 성능을 평가하기 위해 사용한 지표 3가지는 무엇인가요?\"\n",
    "# query = \"국내에서 LLM을 임상시험에 적용한 대표적인 기관과 그 적용 사례를 2가지 이상 말해보세요.\"\n",
    "# query = \"ROUGE 평가에서 Private LLM과 ChatGPT의 Recall 값은 각각 얼마였나요?\"\n",
    "\n",
    "# results = vectorstore.similarity_search(query, k=3) # k는 유사도 검색에서 반환할 상위 문서 개수(top‑k)\n",
    "results = vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "for idx, doc in enumerate(results, start=1):\n",
    "    print(f\"[결과 {idx}]\\n\" + doc.page_content[:300])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99e630fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'본 연구에서 Private LLM 구축을 위해 수집한 문서의 총 페이지 수와 문서 유형별 비율은 어떻게 되나요?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8cdac",
   "metadata": {},
   "source": [
    "#### LCEL 파이프라인은 컴포넌트 간의 명확한 흐름을 강조함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "815659bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='본 연구에서 Private LLM 구축을 위해 수집한 문서의 총 페이지 수는 **11,954 페이지**입니다.\\n\\n문서 유형별 비율은 다음과 같습니다:\\n*   **규제 문서**: 30% (FDA, EMA, PMDA 가이드라인, GCP 문서 등)\\n*   **교육 자료**: 20% (임상시험 수행자 교육 매뉴얼, 온라인 강의 자료 등)\\n*   **프로토콜 및 보고서**: 25% (임상시험 프로토콜, CSR 템플릿 등)\\n*   **의료기기 특화 문서**: 15% (의료기기 임상시험 계획서, 기술문서 등)\\n*   **기타**: 10% (윤리위원회 관련 문서, 환자 동의서 템플릿 등)' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--93c77224-1d19-48ea-89e8-06ced56f2c52-0' usage_metadata={'input_tokens': 2885, 'output_tokens': 726, 'total_tokens': 3611, 'input_token_details': {'cache_read': 2032}, 'output_token_details': {'reasoning': 543}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    '''다음 컨텍스트만 사용해 질문에 답하세요.\n",
    "컨텍스트:{context}\n",
    "\n",
    "질문: {question}\n",
    "'''\n",
    ")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=gemini_api_key)\n",
    "chain = prompt | llm\n",
    "\n",
    "# 관련 문서를 사용한 답변\n",
    "result = chain.invoke({'context': results, 'question': query})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39194f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "본 연구에서 Private LLM 구축을 위해 수집한 문서의 총 페이지 수는 **11,954 페이지**입니다.\n",
      "\n",
      "문서 유형별 비율은 다음과 같습니다:\n",
      "*   **규제 문서**: 30% (FDA, EMA, PMDA 가이드라인, GCP 문서 등)\n",
      "*   **교육 자료**: 20% (임상시험 수행자 교육 매뉴얼, 온라인 강의 자료 등)\n",
      "*   **프로토콜 및 보고서**: 25% (임상시험 프로토콜, CSR 템플릿 등)\n",
      "*   **의료기기 특화 문서**: 15% (의료기기 임상시험 계획서, 기술문서 등)\n",
      "*   **기타**: 10% (윤리위원회 관련 문서, 환자 동의서 템플릿 등)\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61736f6a",
   "metadata": {},
   "source": [
    "#### @chain 데코레이터는 여러 작업을 하나의 함수로 묶어(여러 단계를 하나의 함수 안에 캡슐화) 코드를 구조화함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43658371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "본 연구에서 Private LLM 구축을 위해 수집한 문서의 총 페이지 수는 **11,954 페이지**입니다.\n",
      "\n",
      "문서 유형별 비율은 다음과 같습니다:\n",
      "*   **규제 문서**: 30% (FDA, EMA, PMDA 가이드라인, GCP 문서 등)\n",
      "*   **교육 자료**: 20% (임상시험 수행자 교육 매뉴얼, 온라인 강의 자료 등)\n",
      "*   **프로토콜 및 보고서**: 25% (임상시험 프로토콜, CSR 템플릿 등)\n",
      "*   **의료기기 특화 문서**: 15% (의료기기 임상시험 계획서, 기술문서 등)\n",
      "*   **기타**: 10% (윤리위원회 관련 문서, 환자 동의서 템플릿 등)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def qa(input):\n",
    "    # 관련 문서 검색\n",
    "    docs = vectorstore.similarity_search(input, k=5)\n",
    "    # 프롬프트 포매팅\n",
    "    formatted = prompt.invoke({'context': docs, 'question': input})\n",
    "    # 답변 생성\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "    # return {\"answer\": answer, \"docs\": docs}\n",
    "\n",
    "# 실행\n",
    "result = qa.invoke(query)\n",
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
