{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Windows에서 VLM 사용하기\n",
        "\n",
        "Windows 환경에서는 MLX가 지원되지 않으므로, Hugging Face Transformers를 사용하여 VLM을 구현합니다.\n",
        "\n",
        "## 지원되는 VLM 모델들\n",
        "\n",
        "- **LLaVA (Large Language and Vision Assistant)**: 텍스트와 이미지를 함께 처리\n",
        "- **BLIP-2**: 이미지 캡셔닝과 VQA (Visual Question Answering)\n",
        "- **InstructBLIP**: 지시사항 기반 멀티모달 모델\n",
        "- **MiniGPT-4**: GPT-4와 유사한 멀티모달 능력\n",
        "\n",
        "## 설치 요구사항\n",
        "\n",
        "```bash\n",
        "uv add transformers torch torchvision pillow\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 필요한 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    BlipProcessor, \n",
        "    BlipForConditionalGeneration,\n",
        "    AutoProcessor,\n",
        "    LlavaForConditionalGeneration\n",
        ")\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BLIP-2를 사용한 이미지 캡셔닝 예제\n",
        "\n",
        "BLIP-2는 이미지를 보고 설명을 생성하는 강력한 VLM입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BLIP-2 모델과 프로세서 로드\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# 샘플 이미지 로드 (인터넷에서)\n",
        "url = \"https://images.unsplash.com/photo-1518717758536-85ae29035b6d?w=400\"\n",
        "response = requests.get(url)\n",
        "image = Image.open(BytesIO(response.content))\n",
        "\n",
        "# 이미지 처리 및 캡션 생성\n",
        "inputs = processor(image, return_tensors=\"pt\")\n",
        "out = model.generate(**inputs, max_length=50)\n",
        "caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"이미지 설명: {caption}\")\n",
        "image.show()  # 이미지 표시"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLaVA를 사용한 Visual Question Answering\n",
        "\n",
        "LLaVA는 이미지에 대한 질문에 답할 수 있는 고급 VLM입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LLaVA 모델 로드 (작은 버전 사용)\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "# GPU가 있으면 사용, 없으면 CPU 사용\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "print(f\"모델이 {device}에서 실행됩니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 이미지와 질문 준비\n",
        "url = \"https://images.unsplash.com/photo-1551963831-b3b1ca40c98e?w=400\"\n",
        "response = requests.get(url)\n",
        "image = Image.open(BytesIO(response.content))\n",
        "\n",
        "# 질문 설정\n",
        "question = \"이 이미지에서 무엇을 볼 수 있나요? 자세히 설명해주세요.\"\n",
        "prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
        "\n",
        "# 입력 처리\n",
        "inputs = processor(prompt, image, return_tensors=\"pt\").to(device, torch_dtype)\n",
        "\n",
        "# 답변 생성\n",
        "generate_ids = model.generate(\n",
        "    **inputs, \n",
        "    max_new_tokens=200, \n",
        "    do_sample=True, \n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# 결과 디코딩\n",
        "response = processor.batch_decode(\n",
        "    generate_ids[:, inputs['input_ids'].shape[1]:], \n",
        "    skip_special_tokens=True, \n",
        "    clean_up_tokenization_spaces=False\n",
        ")[0]\n",
        "\n",
        "print(f\"질문: {question}\")\n",
        "print(f\"답변: {response}\")\n",
        "image.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PDF 문서 처리를 위한 VLM 활용\n",
        "\n",
        "PDF 페이지를 이미지로 변환하고 VLM으로 분석하는 예제입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PDF 처리를 위한 추가 라이브러리 (필요시 설치)\n",
        "# uv add pdf2image\n",
        "\n",
        "def analyze_pdf_page_with_vlm(pdf_path, page_number=0):\n",
        "    \"\"\"\n",
        "    PDF 페이지를 VLM으로 분석하는 함수\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from pdf2image import convert_from_path\n",
        "        \n",
        "        # PDF를 이미지로 변환\n",
        "        pages = convert_from_path(pdf_path)\n",
        "        page_image = pages[page_number]\n",
        "        \n",
        "        # VLM으로 페이지 분석\n",
        "        question = \"이 문서 페이지의 내용을 요약하고, 주요 정보를 추출해주세요. 표, 그래프, 이미지가 있다면 설명해주세요.\"\n",
        "        prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
        "        \n",
        "        inputs = processor(prompt, page_image, return_tensors=\"pt\").to(device, torch_dtype)\n",
        "        \n",
        "        generate_ids = model.generate(\n",
        "            **inputs, \n",
        "            max_new_tokens=300, \n",
        "            do_sample=True, \n",
        "            temperature=0.7\n",
        "        )\n",
        "        \n",
        "        response = processor.batch_decode(\n",
        "            generate_ids[:, inputs['input_ids'].shape[1]:], \n",
        "            skip_special_tokens=True, \n",
        "            clean_up_tokenization_spaces=False\n",
        "        )[0]\n",
        "        \n",
        "        return response, page_image\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"pdf2image 라이브러리가 필요합니다. 'uv add pdf2image'로 설치해주세요.\")\n",
        "        return None, None\n",
        "\n",
        "# 사용 예제 (PDF 파일이 있는 경우)\n",
        "# analysis, image = analyze_pdf_page_with_vlm(\"sample.pdf\", 0)\n",
        "# if analysis:\n",
        "#     print(f\"페이지 분석 결과: {analysis}\")\n",
        "#     image.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG 시스템과의 통합\n",
        "\n",
        "VLM을 RAG 파이프라인에 통합하는 방법:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VLMRAGProcessor:\n",
        "    \"\"\"\n",
        "    VLM을 활용한 RAG 처리기\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vlm_model, vlm_processor):\n",
        "        self.vlm_model = vlm_model\n",
        "        self.vlm_processor = vlm_processor\n",
        "        self.device = next(vlm_model.parameters()).device\n",
        "        \n",
        "    def extract_visual_content(self, image, context_question=None):\n",
        "        \"\"\"\n",
        "        이미지에서 텍스트 정보를 추출\n",
        "        \"\"\"\n",
        "        if context_question is None:\n",
        "            context_question = \"이 이미지의 모든 텍스트, 표, 차트, 그래프 내용을 자세히 설명해주세요. 구조와 데이터를 포함해서 설명해주세요.\"\n",
        "        \n",
        "        prompt = f\"USER: <image>\\n{context_question}\\nASSISTANT:\"\n",
        "        \n",
        "        inputs = self.vlm_processor(prompt, image, return_tensors=\"pt\").to(self.device)\n",
        "        \n",
        "        generate_ids = self.vlm_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=500,\n",
        "            do_sample=True,\n",
        "            temperature=0.3  # 더 일관된 결과를 위해 낮은 온도\n",
        "        )\n",
        "        \n",
        "        response = self.vlm_processor.batch_decode(\n",
        "            generate_ids[:, inputs['input_ids'].shape[1]:],\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=False\n",
        "        )[0]\n",
        "        \n",
        "        return response\n",
        "    \n",
        "    def answer_visual_question(self, image, question):\n",
        "        \"\"\"\n",
        "        이미지 기반 질문 답변\n",
        "        \"\"\"\n",
        "        prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
        "        \n",
        "        inputs = self.vlm_processor(prompt, image, return_tensors=\"pt\").to(self.device)\n",
        "        \n",
        "        generate_ids = self.vlm_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=300,\n",
        "            do_sample=True,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        \n",
        "        response = self.vlm_processor.batch_decode(\n",
        "            generate_ids[:, inputs['input_ids'].shape[1]:],\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=False\n",
        "        )[0]\n",
        "        \n",
        "        return response\n",
        "\n",
        "# VLM RAG 프로세서 초기화\n",
        "vlm_rag = VLMRAGProcessor(model, processor)\n",
        "\n",
        "print(\"VLM RAG 프로세서가 준비되었습니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 성능 최적화 팁\n",
        "\n",
        "### 1. GPU 메모리 관리\n",
        "```python\n",
        "# GPU 메모리 정리\n",
        "torch.cuda.empty_cache()\n",
        "```\n",
        "\n",
        "### 2. 배치 처리\n",
        "여러 이미지를 한 번에 처리하여 효율성 향상\n",
        "\n",
        "### 3. 모델 양자화\n",
        "```python\n",
        "# 4비트 양자화로 메모리 사용량 감소\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "```\n",
        "\n",
        "## 결론\n",
        "\n",
        "Windows에서는 MLX 대신 Hugging Face Transformers를 사용하여 VLM을 구현할 수 있습니다. 주요 장점:\n",
        "\n",
        "1. **광범위한 모델 지원**: LLaVA, BLIP-2, InstructBLIP 등\n",
        "2. **GPU 가속**: CUDA 지원으로 빠른 추론\n",
        "3. **유연한 통합**: 기존 RAG 시스템과 쉬운 통합\n",
        "4. **활발한 커뮤니티**: 지속적인 업데이트와 지원\n",
        "\n",
        "이 방법으로 Windows에서도 강력한 VLM 기반 RAG 시스템을 구축할 수 있습니다."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
