import streamlit as st
from langchain_upstage import UpstageDocumentParseLoader
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
import tempfile
import os
import json
import random
import re
import shutil
from dotenv import load_dotenv
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
gemini_api_key = os.getenv("GEMINI_API_KEY")
upstage_api_key = os.getenv("UPSTAGE_API_KEY")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history_for_chain" not in st.session_state:
    st.session_state.chat_history_for_chain = ChatMessageHistory()
if "messages" not in st.session_state:
    st.session_state.messages = []
if "pdf_processed" not in st.session_state:
    st.session_state.pdf_processed = False
if "current_question" not in st.session_state:
    st.session_state.current_question = None
if "wrong_answers" not in st.session_state:
    st.session_state.wrong_answers = []
if "is_retest" not in st.session_state:
    st.session_state.is_retest = False

# Langchain ëª¨ë¸ ë° FAISS DB ì´ˆê¸°í™”
chat = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=gemini_api_key)
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", 
    google_api_key=gemini_api_key, 
    transport='rest' # Streamlitì˜ ë™ê¸°ì ì¸ í™˜ê²½ê³¼ í˜¸í™˜ë˜ë„ë¡ ì„¤ì •(GoogleGenerativeAIEmbeddingsëŠ” ê¸°ë³¸ê°’ì€ ë¹„ë™ê¸°)
)
db_path = "faiss_index"

@st.cache_resource
def get_vector_store():
    # FAISS ì¸ë±ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë¡œë“œ
    if os.path.exists(db_path):
        try:
            return FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
        except Exception as e:
            st.error(f"ê¸°ì¡´ FAISS DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    # íŒŒì¼ì´ ì—†ìœ¼ë©´ Noneì„ ë°˜í™˜
    return None

st.session_state.vector_store = get_vector_store()

# --- í•¨ìˆ˜ ì •ì˜ ---
def load_and_embed_pdf(pdf_path):
    """PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  FAISSì— ì„ë² ë”©í•©ë‹ˆë‹¤."""
    loader = UpstageDocumentParseLoader(pdf_path, split='page')
    docs = loader.load()
    
    if not docs:
        st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ ìœ íš¨í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return []

    # í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°ë¥¼ ì‚¬ìš©í•´ ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ”
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(docs)

    if not split_docs:
        st.error("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì•„ ë¬¸ì œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    # FAISSì— ì„ë² ë”© ë° ì €ì¥
    if st.session_state.vector_store:
        # DBê°€ ì¡´ì¬í•˜ë©´ ê¸°ì¡´ DBì— ë¬¸ì„œ ì¶”ê°€
        st.session_state.vector_store.add_documents(split_docs)
    else:
        # DBê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        st.session_state.vector_store = FAISS.from_documents(split_docs, embeddings)
    
    st.session_state.vector_store.save_local(db_path)
    
    # ë””ë²„ê¹…ì„ ìœ„í•´ ë¬¸ì„œ ìˆ˜ë¥¼ ì¶œë ¥
    st.info(f"ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(split_docs)}ê°œ")
    
    return [doc.page_content for doc in split_docs]


def question_generator():
    """ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ PDF ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # FAISS DBì— ì˜ë¯¸ ìˆëŠ” ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸
    if not st.session_state.vector_store:
        return None
    docs = st.session_state.vector_store.similarity_search("documents", k=1)
    if not docs or docs[0].page_content == "":
        return None
    
    if st.session_state.wrong_answers and random.random() < 0.5:
        st.session_state.is_retest = True
        return random.choice(st.session_state.wrong_answers)
    else:
        st.session_state.is_retest = False
        
        # ë¬¸ì œ ìƒì„±ìš© ì»¨í…ìŠ¤íŠ¸ ì„ íƒ
        if st.session_state.get("latest_pdf_content") and random.random() < 0.5:
            pdf_context = st.session_state.get("latest_pdf_content")
        else:
            retriever = st.session_state.vector_store.as_retriever()
            docs = retriever.invoke("random documents", k=10)
            pdf_context = " ".join([doc.page_content for doc in docs])
        
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ë‹¹ì‹ ì€ ì œê³µëœ í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ ìˆê³  ë§¥ë½ì ìœ¼ë¡œ ê´€ë ¨ëœ ê°ê´€ì‹ ë¬¸ì œ(4ì§€ì„ ë‹¤)ë¥¼ ìƒì„±í•˜ëŠ” ê³ ê¸‰ ì§ˆë¬¸ ìƒì„±ê¸°ì…ë‹ˆë‹¤.
                    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ 1ê°œì˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”. ë‹µë³€ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
                    ê²°ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
                    {{
                        "question": "ë¬¸ì œ ë‚´ìš©",
                        "options": ["1. ë³´ê¸°1", "2. ë³´ê¸°2", "3. ë³´ê¸°3", "4. ë³´ê¸°4"],
                        "answer": "ì •ë‹µ ë²ˆí˜¸ (1~4)",
                        "explanation": "ë¬¸ì œì— ëŒ€í•œ í•´ì„¤"
                    }}
                    \n\n
                    {context}""",
                ),
                (
                    "human",
                    "{input}"
                )
            ]
        )
        chain = prompt | chat
        try:
            ai_response = chain.invoke({
                "context": pdf_context, "input": "4ì§€ì„ ë‹¤ 1ë¬¸í•­ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”."
            }).content
            
            json_match = re.search(r'\{.*\}', ai_response, re.DOTALL)
            
            if json_match:
                json_string = json_match.group(0)
                response_json = json.loads(json_string)
                return response_json
            else:
                st.error("AI ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                return None
            
        except json.JSONDecodeError as e:
            st.error("JSON íŒŒì‹± ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            return None

def display_question(q_data):
    """ì§ˆë¬¸ ë°ì´í„°ë¥¼ UIì— í‘œì‹œí•©ë‹ˆë‹¤."""
    st.session_state.messages.append({"role": "assistant", "content": q_data['question']})
    st.session_state.messages.append({"role": "assistant", "content": "\n".join(q_data['options'])})

    st.write(q_data['question'])
    for option in q_data['options']:
        st.write(option)

def check_answer_and_proceed(user_message):
    """ì‚¬ìš©ì ë‹µë³€ì„ í™•ì¸í•˜ê³  ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•©ë‹ˆë‹¤."""
    q_data = st.session_state.current_question
    if not q_data:
        return "ë¬¸ì œê°€ ì¶œì œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDFë¥¼ ì œì¶œí•˜ì—¬ ì‹œì‘í•´ì£¼ì„¸ìš”."

    try:
        user_answer = int(user_message.strip())
        correct_answer = int(q_data['answer'])

        if user_answer == correct_answer:
            response = "ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰"
            if st.session_state.is_retest:
                response += " ì´ì „ì— í‹€ë ¸ë˜ ë¬¸ì œì˜€ëŠ”ë°, ì˜ ë§ì¶”ì…¨ë„¤ìš”! ğŸ‘"
                st.session_state.wrong_answers = [q for q in st.session_state.wrong_answers if q['question'] != q_data['question']]
            
            st.session_state.messages.append({"role": "assistant", "content": response})

            # ìƒˆë¡œìš´ ë¬¸ì œ ìƒì„± ë° ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            new_q_data = question_generator()
            st.session_state.current_question = new_q_data
            
            if new_q_data:
                # ìƒˆë¡œìš´ ë¬¸ì œë„ ì±— ë©”ì‹œì§€ì— ì¶”ê°€í•˜ì—¬ í™”ë©´ì— ì¶œë ¥
                st.session_state.messages.append({"role": "assistant", "content": new_q_data['question']})
                st.session_state.messages.append({"role": "assistant", "content": "\n".join(new_q_data['options'])})

            return response
        else:
            response = f"ì•„ì‰½ì§€ë§Œ ì •ë‹µì´ ì•„ë‹™ë‹ˆë‹¤. ì •ë‹µì€ {correct_answer}ë²ˆì…ë‹ˆë‹¤. ğŸ˜…\n\n**í•´ì„¤:**\n{q_data['explanation']}"
            
            if q_data not in st.session_state.wrong_answers:
                st.session_state.wrong_answers.append(q_data)
            
            st.session_state.messages.append({"role": "assistant", "content": response})
            return response

    except ValueError:
        return None

def general_response_generator(user_message):
    """RAGë¥¼ ì ìš©í•œ ì¼ë°˜ì ì¸ ëŒ€í™” ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    # FAISS DBì—ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ(ì²­í¬)ë¥¼ ê²€ìƒ‰
    if not st.session_state.vector_store:
        return "ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
        
    retriever = st.session_state.vector_store.as_retriever()
    docs = retriever.invoke(user_message)
    retrieved_context = "\n".join([doc.page_content for doc in docs])

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
                ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì œê³µëœ í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
                ë§Œì•½ ì œê³µëœ í…ìŠ¤íŠ¸ì— ë‹µì´ ì—†ë‹¤ë©´, "ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ë§í•´ì£¼ì„¸ìš”.
                í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
                ì œê³µëœ í…ìŠ¤íŠ¸: {context}""",
            ),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ]
    )
    chain = prompt | chat

    chain_with_message_history = RunnableWithMessageHistory(
        chain,
        lambda session_id: st.session_state.chat_history_for_chain,
        input_messages_key="input",
        history_messages_key="chat_history",
    )

    response = chain_with_message_history.invoke(
        {"input": user_message, "context": retrieved_context},
        {"configurable": {"session_id": "123"}},
    )
    return response.content

# --- Streamlit UI êµ¬ì„± ---
st.set_page_config(page_title="PDFë¡œ AIì™€ ê³µë¶€í•˜ê¸°", layout="wide")

st.markdown("""
<style>
.header-container {
    text-align: center;
    max-width: 1000px;
    margin: 10px auto;
}
.header-container h1 {
    font-size: 2.5em;
}
.header-container p {
    font-size: 1em;
    margin-bottom: 20px;
}
</style>
""", unsafe_allow_html=True)

st.markdown("""
<div class="header-container">
    <h1>ğŸ“– PDFë¡œ AIì™€ ê³µë¶€í•˜ê¸° ğŸ“–</h1>
    <p>ğŸ’­ í•™ìŠµìë£Œ PDFë¥¼ ì—…ë¡œë“œ í•´ë³´ì„¸ìš”. AIê°€ ìë£Œì—ì„œ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ ì¤„ê±°ì˜ˆìš”. ì •ë‹µì„ ë§í˜€ ë³´ì„¸ìš”.</p>
</div>
""", unsafe_allow_html=True)

# FAISS DBê°€ ì¡´ì¬í•˜ê³ , ì•„ì§ PDFê°€ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ë‹¤ë©´(ì²« ì‹¤í–‰), ë¬¸ì œ ì¶œì œ
if st.session_state.vector_store and not st.session_state.pdf_processed:
    try:
        with st.spinner('FAISS DBì—ì„œ ë¬¸ì œë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'):
            q_data = question_generator()
            st.session_state.current_question = q_data

        if q_data:
            display_question(q_data)
            st.session_state.pdf_processed = True
        else:
            # question_generatorê°€ Noneì„ ë°˜í™˜í–ˆì„ ë•Œë§Œ ê²½ê³  ë©”ì‹œì§€ë¥¼ ì¶œë ¥
            st.warning("ë¬¸ì œê°€ ì¶œì œë  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    except Exception as e:
        st.error(f"ë¬¸ì œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.info("ìƒˆë¡œìš´ PDFë¥¼ ì—…ë¡œë“œí•˜ì—¬ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜, 'faiss_index' í´ë”ë¥¼ ì‚­ì œí•˜ê³  ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

# --- PDF ì—…ë¡œë“œ ë° ì²˜ë¦¬ ---
pdf_file = st.file_uploader("Upload a PDF", type="pdf", label_visibility="collapsed")
submit_button = st.button("PDF ì œì¶œ", type="primary")

# PDF ì œì¶œ ë²„íŠ¼ í´ë¦­ ì‹œ ë™ì‘
if submit_button and pdf_file is not None:
    # ê¸°ì¡´ FAISS DB ì´ˆê¸°í™” (ìƒˆë¡œìš´ PDF ì œì¶œ ì‹œ, ê¸°ì¡´ ë°ì´í„° ì‚­ì œ)
    if os.path.exists(db_path):
        st.warning("ê¸°ì¡´ FAISS DBê°€ ì¡´ì¬í•˜ì—¬, ìƒˆë¡œìš´ PDFë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ì´ì „ ë‚´ìš©ì€ ì‚­ì œë©ë‹ˆë‹¤.")
        
        # --- ìˆ˜ì •ëœ ë¶€ë¶„ ---
        # 1. FAISS í´ë”ë¥¼ ì‚­ì œ
        shutil.rmtree(db_path)
        # 2. ìºì‹œëœ FAISS ì¸ìŠ¤í„´ìŠ¤ë„ í•¨ê»˜ ì‚­ì œ
        get_vector_store.clear()
        
        st.session_state.vector_store = None
        st.session_state.current_question = None
        st.session_state.messages = []
        st.session_state.wrong_answers = []
    # ------------------
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_pdf:
        temp_pdf.write(pdf_file.read())
        temp_path = temp_pdf.name
    
    st.session_state.pdf_path = temp_path
    st.session_state.pdf_processed = False

    with st.spinner('PDFë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...'):
        load_and_embed_pdf(st.session_state.pdf_path)
        q_data = question_generator()
        st.session_state.current_question = q_data

    if q_data:
        display_question(q_data)
        st.session_state.pdf_processed = True
    else:
        st.warning("ë¬¸ì œê°€ ì¶œì œë  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì˜ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# --- ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("AIê°€ ì¶œì œí•œ ë¬¸ì œì— ë‹µì„ í•˜ê±°ë‚˜ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”."):
    if not st.session_state.pdf_processed:
        st.warning("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'PDF ì œì¶œ' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        with st.chat_message("assistant"):
            with st.spinner("ìƒê° ì¤‘..."):
                answer_check_result = check_answer_and_proceed(prompt)

                if answer_check_result:
                    st.write(answer_check_result)
                    
                    new_q_data = question_generator()
                    st.session_state.current_question = new_q_data
                    
                    if new_q_data:
                        st.write("---")
                        display_question(new_q_data)
                else:
                    full_response = general_response_generator(prompt)
                    st.write(full_response)
                    st.session_state.messages.append({"role": "assistant", "content": full_response})

# --- PDF íŒŒì¼ ì •ë¦¬ ---
if st.session_state.get("pdf_path") and os.path.exists(st.session_state.pdf_path):
    os.unlink(st.session_state.pdf_path)
