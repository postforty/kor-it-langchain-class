import streamlit as st
from langchain_upstage import UpstageDocumentParseLoader
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
import tempfile
import os
import json
import random
import re
from dotenv import load_dotenv
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
gemini_api_key = os.getenv("GEMINI_API_KEY")
upstage_api_key = os.getenv("UPSTAGE_API_KEY")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history_for_chain" not in st.session_state:
    st.session_state.chat_history_for_chain = ChatMessageHistory()
if "pdf_context" not in st.session_state:
    st.session_state.pdf_context = ""
if "messages" not in st.session_state:
    st.session_state.messages = []
if "pdf_processed" not in st.session_state:
    st.session_state.pdf_processed = False
if "current_question" not in st.session_state:
    st.session_state.current_question = None
if "wrong_answers" not in st.session_state:
    st.session_state.wrong_answers = []
if "is_retest" not in st.session_state:
    st.session_state.is_retest = False

chat = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=gemini_api_key)

# --- í•¨ìˆ˜ ì •ì˜ ---
def load_and_parse_pdf(pdf_path):
    """PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  íŒŒì‹±í•˜ì—¬ ì„¸ì…˜ ìƒíƒœì— ì €ì¥í•©ë‹ˆë‹¤."""
    loader = UpstageDocumentParseLoader(pdf_path, split='page')
    pages = loader.load()
    pdf_context = ''
    for page in pages:
        pdf_context += page.page_content
    st.session_state.pdf_context = pdf_context

def question_generator():
    """ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ PDF ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    # í‹€ë ¸ë˜ ë¬¸ì œê°€ ìˆìœ¼ë©´ ë‹¤ì‹œ ì¶œì œ
    if st.session_state.wrong_answers and random.random() < 0.5:  # 50% í™•ë¥ ë¡œ ì¬ì¶œì œ
        st.session_state.is_retest = True
        return random.choice(st.session_state.wrong_answers)
    else:
        st.session_state.is_retest = False
        pdf_context = st.session_state.pdf_context
        
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ë‹¹ì‹ ì€ ì œê³µëœ í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ ìˆê³  ë§¥ë½ì ìœ¼ë¡œ ê´€ë ¨ëœ ê°ê´€ì‹ ë¬¸ì œ(4ì§€ì„ ë‹¤)ë¥¼ ìƒì„±í•˜ëŠ” ê³ ê¸‰ ì§ˆë¬¸ ìƒì„±ê¸°ì…ë‹ˆë‹¤.
                    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ 1ê°œì˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”. ë‹µë³€ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
                    ê²°ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
                    {{
                        "question": "ë¬¸ì œ ë‚´ìš©",
                        "options": ["1. ë³´ê¸°1", "2. ë³´ê¸°2", "3. ë³´ê¸°3", "4. ë³´ê¸°4"],
                        "answer": "ì •ë‹µ ë²ˆí˜¸ (1~4)",
                        "explanation": "ë¬¸ì œì— ëŒ€í•œ í•´ì„¤"
                    }}
                    \n\n
                    {context}""",
                ),
                (
                    "human",
                    "{input}"
                )
            ]
        )
        chain = prompt | chat
        try:
            ai_response = chain.invoke({
                "context": pdf_context, "input": "4ì§€ì„ ë‹¤ 1ë¬¸í•­ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”."
            }).content
            
            # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì¤‘ê´„í˜¸ {}ë¡œ ì‹œì‘í•˜ê³  ëë‚˜ëŠ” JSON ë¬¸ìì—´ì„ ì°¾ìŒ
            json_match = re.search(r'\{.*\}', ai_response, re.DOTALL)
            
            if json_match:
                json_string = json_match.group(0)
                response_json = json.loads(json_string)
                return response_json
            else:
                st.error("AI ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                return None
            
        except json.JSONDecodeError as e:
            st.error("JSON íŒŒì‹± ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            print(e)
            return None

def display_question(q_data):
    """ì§ˆë¬¸ ë°ì´í„°ë¥¼ UIì— í‘œì‹œí•©ë‹ˆë‹¤."""
    st.session_state.messages.append({"role": "assistant", "content": q_data['question']})
    st.session_state.messages.append({"role": "assistant", "content": "\n".join(q_data['options'])})

    # UIì— í‘œì‹œ
    st.write(q_data['question'])
    for option in q_data['options']:
        st.write(option)

def check_answer_and_proceed(user_message):
    """ì‚¬ìš©ì ë‹µë³€ì„ í™•ì¸í•˜ê³  ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•©ë‹ˆë‹¤."""
    q_data = st.session_state.current_question
    if not q_data:
        return "ë¬¸ì œê°€ ì¶œì œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDFë¥¼ ì œì¶œí•˜ì—¬ ì‹œì‘í•´ì£¼ì„¸ìš”."

    # ìˆ«ìê°€ ì•„ë‹Œ ë‹¤ë¥¸ ì…ë ¥ì´ë©´ ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬
    try:
        user_answer = int(user_message.strip())
        correct_answer = int(q_data['answer'])

        if user_answer == correct_answer:
            response = "ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰"
            if st.session_state.is_retest:
                response += " ì´ì „ì— í‹€ë ¸ë˜ ë¬¸ì œì˜€ëŠ”ë°, ì˜ ë§ì¶”ì…¨ë„¤ìš”! ğŸ‘"
                # ì •ë‹µ ë§ì¶˜ ë¬¸ì œëŠ” ì˜¤ë‹µ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œê±°
                st.session_state.wrong_answers = [q for q in st.session_state.wrong_answers if q['question'] != q_data['question']]
            
            st.session_state.messages.append({"role": "assistant", "content": response})
            return response
        else:
            response = f"ì•„ì‰½ì§€ë§Œ ì •ë‹µì´ ì•„ë‹™ë‹ˆë‹¤. ì •ë‹µì€ {correct_answer}ë²ˆì…ë‹ˆë‹¤. ğŸ˜…\n\n**í•´ì„¤:**\n{q_data['explanation']}"
            
            # í‹€ë¦° ë¬¸ì œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
            if q_data not in st.session_state.wrong_answers:
                st.session_state.wrong_answers.append(q_data)
            
            st.session_state.messages.append({"role": "assistant", "content": response})
            return response

    except ValueError: # ì‚¬ìš©ìê°€ ìˆ«ìê°€ ì•„ë‹Œ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í–ˆì„ ê²½ìš°
        return None # ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ë¡œ ë„˜ê¹€

def general_response_generator(user_message):
    """ì¼ë°˜ì ì¸ ëŒ€í™”ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """
                ì‚¬ìš©ìê°€ ì˜¬ë°”ë¥´ê²Œ ëŒ€ë‹µí–ˆë‹¤ë©´, ë‹¤ìŒ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.
                ì‚¬ìš©ìê°€ ë‹µë³€í•˜ì§€ ì•Šì•˜ê±°ë‚˜ í‹€ë ¸ì„ ê²½ìš°, ì œê³µëœ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.
                í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
                ì œê³µëœ í…ìŠ¤íŠ¸: {context}""",
            ),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ]
    )
    chain = prompt | chat

    chain_with_message_history = RunnableWithMessageHistory(
        chain,
        lambda session_id: st.session_state.chat_history_for_chain,
        input_messages_key="input",
        history_messages_key="chat_history",
    )

    response = chain_with_message_history.invoke(
        {"input": user_message, "context": st.session_state.pdf_context},
        {"configurable": {"session_id": "123"}},
    )
    return response.content

# --- Streamlit UI êµ¬ì„± ---
st.set_page_config(page_title="PDFë¡œ AIì™€ ê³µë¶€í•˜ê¸°", layout="wide")

st.markdown("""
<style>
.header-container {
    text-align: center;
    max-width: 1000px;
    margin: 10px auto;
}
.header-container h1 {
    font-size: 2.5em;
}
.header-container p {
    font-size: 1em;
    margin-bottom: 20px;
}
</style>
""", unsafe_allow_html=True)

st.markdown("""
<div class="header-container">
    <h1>ğŸ“– PDFë¡œ AIì™€ ê³µë¶€í•˜ê¸° ğŸ“–</h1>
    <p>ğŸ’­ í•™ìŠµìë£Œ PDFë¥¼ ì—…ë¡œë“œ í•´ë³´ì„¸ìš”. AIê°€ ìë£Œì—ì„œ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ ì¤„ê±°ì˜ˆìš”. ì •ë‹µì„ ë§í˜€ ë³´ì„¸ìš”.</p>
</div>
""", unsafe_allow_html=True)

# --- PDF ì—…ë¡œë“œ ë° ì²˜ë¦¬ ---
pdf_file = st.file_uploader("Upload a PDF", type="pdf", label_visibility="collapsed")
submit_button = st.button("PDF ì œì¶œ", type="primary")

# PDF ì œì¶œ ë²„íŠ¼ í´ë¦­ ì‹œ ë™ì‘
if submit_button and pdf_file is not None and not st.session_state.pdf_processed:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_pdf:
        temp_pdf.write(pdf_file.read())
        temp_path = temp_pdf.name
    
    st.session_state.pdf_path = temp_path
    
    with st.spinner('PDFë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...'):
        load_and_parse_pdf(st.session_state.pdf_path)
        q_data = question_generator()
        st.session_state.current_question = q_data

    if q_data:
        display_question(q_data)
        st.session_state.pdf_processed = True

# --- ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("AIê°€ ì¶œì œí•œ ë¬¸ì œì— ë‹µì„ í•˜ê±°ë‚˜ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”."):
    if not st.session_state.pdf_processed:
        st.warning("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'PDF ì œì¶œ' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        with st.chat_message("assistant"):
            with st.spinner("ìƒê° ì¤‘..."):
                # ì‚¬ìš©ì ì…ë ¥ì´ ë¬¸ì œì— ëŒ€í•œ ë‹µë³€ì¸ì§€ í™•ì¸
                answer_check_result = check_answer_and_proceed(prompt)

                if answer_check_result: # ë‹µë³€ ì²˜ë¦¬ ë¡œì§ ì‹¤í–‰
                    st.write(answer_check_result)
                    
                    # ì •ë‹µ/ì˜¤ë‹µ í›„ ë‹¤ìŒ ë¬¸ì œ ì¶œì œ
                    new_q_data = question_generator()
                    st.session_state.current_question = new_q_data
                    
                    if new_q_data:
                        # ë‹¤ìŒ ë¬¸ì œ UIì— í‘œì‹œ
                        st.write("---") # ì‹œê°ì  êµ¬ë¶„
                        display_question(new_q_data)
                else: # ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬
                    full_response = general_response_generator(prompt)
                    st.write(full_response)
                    st.session_state.messages.append({"role": "assistant", "content": full_response})

# --- PDF íŒŒì¼ ì •ë¦¬ ---
if st.session_state.get("pdf_path") and os.path.exists(st.session_state.pdf_path):
    os.unlink(st.session_state.pdf_path)